#!/usr/bin/env python
# coding: utf-8

# In[1]:


import sklearn.datasets
import numpy as np


# In[3]:


cancer_ds = sklearn.datasets.load_breast_cancer()


# In[4]:


#lets organise asX and Y
X = cancer_ds.data
Y = cancer_ds.target


# In[5]:


print(X)
print(Y)


# In[6]:


print(X.shape, Y.shape)


# In[7]:


#get dataset inside a dataframe
import pandas as pd
data = pd.DataFrame(cancer_ds.data, columns = cancer_ds.feature_names)


# In[8]:


#rename the Y variable as Target
data['class'] = cancer_ds.target


# In[9]:


#examine the first few records
data.head()


# In[10]:


data.describe()


# In[11]:


#how many malignant and benign?
print(data['class'].value_counts()) #binary variable either 1 or 0


# In[12]:


#know the target names
print(cancer_ds.target_names)


# In[13]:


data.groupby('class').mean()


# In[14]:


data.groupby('mean area').mean()


# train test split

# In[15]:


from sklearn.model_selection import train_test_split


# In[16]:


X = data.drop('class', axis=1)
Y = data['class']


# In[73]:


X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.1, random_state = 42)


# In[56]:


print(Y.shape, Y_train.shape, Y_test.shape)


# # random_state

# If there is no random_state provided the system will use a random_state that is generated internally. So, when we run the program multiple times we might see different train/test data points and the behavior will be unpredictable. In case, we have an issue with our model we will not be able to recreate it as we do not know the random number that was generated when we ran the program.

# In[47]:


Y_train # Without random state


# In[60]:


Y_train # Another epoch Without random state


# To avoid getting different values for train and test every time, we use random_state. This will also have an impact on the evaluation metrics as if the random_state is not selected the values from the evaluation will differ. The values of random_state variable can be any integer, 0 and 1 are the most common values used. You can select your own value. Assume it as a shuffle of cards were, after the first shuffle that's random state 1 the after the second shuffle that's random state 2 etc.

# In[57]:


Y_train # With random_state = 10


# In[58]:


Y_train # Another epoch With random_state = 10, we will get the same set of values


# In[62]:


Y_train # With random_state = 42


# In[63]:


Y_train # Another epoch With random_state = 42


# # Stratify

# Stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.
# 
# For example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify = y will make sure that your random split has 25% of 0's and 75% of 1's.
# 
# Suppose a scenario in which you have 100 binary(0,1) datasets of a cancer patient and you have splitted it into 80% as train_data and 20% as test_data. And by default in test_data, you get 15 values as " 1 " and 5 values as 0, So it will be a quite biased data_set. In order to avoid this situation, we use "stratify" to provide a set of common relationship between training and testing dataset

# In[68]:


Y.value_counts() # Ratio of 1 is to 0 in Y is 1.6


# With Stratify

# In[70]:


Y_train.value_counts() #with startify
                      # Ratio of 1 is to 0 in Y_train is 1.6


# In[71]:


Y_test.value_counts() # Ratio of 1 is to 0 in Y_test is 1.6


# Without Stratify

# In[74]:


Y_train.value_counts() #without startify
                      # Ratio of 1 is to 0 in Y is 1.6


# In[75]:


Y_test.value_counts() # Ratio of 1 is to 0 in Y is 2.3


# If we dont use stratify variable then we get the composition of the train and test sets differ, and this is not desirable and it makes the data biased
